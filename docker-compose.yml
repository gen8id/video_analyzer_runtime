services:
  video_analyzer:
    build:
      context: .
      dockerfile: Dockerfile
    image: aicompanion/video_analyzer:latest
    container_name: video_analyzer

    # GPU 설정
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    ports:
      - "7860:7860"   # Gradio UI

    # 볼륨 마운트 (호스트 ↔ 컨테이너)
    volumes:
      - ./models:/workspace/video_analyzer/models
      - ./videos:/workspace/video_analyzer/videos
      - ./outputs:/workspace/video_analyzer/outputs

    environment:
      # GPU 선택 (필요 시 GPU ID 지정)
      # - CUDA_VISIBLE_DEVICES=0,1,2,3    
      - HF_HOME=/workspace/video_analyzer/models
      - HF_HUB_CACHE=/workspace/video_analyzer/models
      - PYTHONUNBUFFERED=1
      - TZ=Asia/Seoul
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      - TMPDIR=/dev/shm

    stdin_open: true
    tty: true
    restart: unless-stopped
    shm_size: "24gb"

    working_dir: /workspace/video_analyzer

    # GPU 자동 탐색, 기본 0번 GPU
    command: ["python", "run-gradio.py"] 

    # 여러개 GPU가 있는 경우 특정 GPU 번호를 지정하거나 gradio live를 사용하려는 경우
    # command: ["python", "run-gradio.py", " --device 3", "--share"] 